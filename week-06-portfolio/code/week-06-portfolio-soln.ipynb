{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME_DIR: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering\n",
      "DATASETS_DIR: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets\n",
      "DATA_DIR: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11\n",
      "TRAIN_CSV_FILE: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/bounding_boxes/train_labels.csv\n",
      "TRAIN_IMAGES_DIR: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/train\n",
      "TRAIN_LABELS_DIR: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/labels/train\n",
      "TEST_CSV_FILE: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/bounding_boxes/test_labels.csv\n",
      "TEST_IMAGES_DIR: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test\n",
      "TEST_LABELS_DIR: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/labels/test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "HOME_DIR = '/Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering'\n",
    "os.chdir(HOME_DIR)\n",
    "\n",
    "DATASETS_DIR = f'{HOME_DIR}/datasets'\n",
    "DATA_DIR = f'{DATASETS_DIR}/11'\n",
    "\n",
    "TRAIN_CSV_FILE = f'{DATA_DIR}/bounding_boxes/train_labels.csv'\n",
    "TRAIN_IMAGES_DIR = f'{DATA_DIR}/images/train'\n",
    "TRAIN_LABELS_DIR = f'{DATA_DIR}/labels/train'\n",
    "SELECTED_TRAIN_IMAGES_DIR= f'{DATA_DIR}/images/train_selected'\n",
    "SELECTED_TRAIN_LABELS_DIR = f'{DATA_DIR}/labels/train_selected'\n",
    "\n",
    "\n",
    "TEST_CSV_FILE = f'{DATA_DIR}/bounding_boxes/test_labels.csv'\n",
    "TEST_IMAGES_DIR = f'{DATA_DIR}/images/test'\n",
    "TEST_LABELS_DIR = f'{DATA_DIR}/labels/test'\n",
    "SELECTED_TEST_IMAGES_DIR = f'{DATA_DIR}/images/test_selected'\n",
    "SELECTED_TEST_LABELS_DIR = f'{DATA_DIR}/labels/test_selected'\n",
    "\n",
    "OUTPUT_IMAGES_DIR = 'week-06-portfolio/evaluation_images'\n",
    "\n",
    "yaml_file_path = f'{DATA_DIR}/graffiti.yaml'\n",
    "\n",
    "print(f'HOME_DIR: {HOME_DIR}')\n",
    "print(f'DATASETS_DIR: {DATASETS_DIR}')\n",
    "print(f'DATA_DIR: {DATA_DIR}')\n",
    "print(f'TRAIN_CSV_FILE: {TRAIN_CSV_FILE}')\n",
    "print(f'TRAIN_IMAGES_DIR: {TRAIN_IMAGES_DIR}')\n",
    "print(f'TRAIN_LABELS_DIR: {TRAIN_LABELS_DIR}')\n",
    "print(f'TEST_CSV_FILE: {TEST_CSV_FILE}')\n",
    "print(f'TEST_IMAGES_DIR: {TEST_IMAGES_DIR}')\n",
    "print(f'TEST_LABELS_DIR: {TEST_LABELS_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import platform\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision.ops import box_iou\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision.ops import box_iou\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if platform.system() == 'Darwin':\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_annotations(csv_file, images_dir, output_dir, class_mapping):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    grouped = df.groupby('filename')\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for filename, group in tqdm(grouped, desc=f'Converting annotations for {csv_file}'):\n",
    "        image_path = os.path.join(images_dir, filename)\n",
    "        if not os.path.exists(image_path):\n",
    "            continue  # Skip if image does not exist\n",
    "\n",
    "        img_width = group.iloc[0]['width']\n",
    "        img_height = group.iloc[0]['height']\n",
    "\n",
    "        annotations = []\n",
    "        for _, row in group.iterrows():\n",
    "            class_id = class_mapping[row['class']]\n",
    "            xmin = row['xmin']\n",
    "            ymin = row['ymin']\n",
    "            xmax = row['xmax']\n",
    "            ymax = row['ymax']\n",
    "\n",
    "            # Convert to YOLO format\n",
    "            x_center = ((xmin + xmax) / 2) / img_width\n",
    "            y_center = ((ymin + ymax) / 2) / img_height\n",
    "            bbox_width = (xmax - xmin) / img_width\n",
    "            bbox_height = (ymax - ymin) / img_height\n",
    "\n",
    "            annotations.append(f\"{class_id} {x_center} {y_center} {bbox_width} {bbox_height}\")\n",
    "\n",
    "        # Write annotations to file\n",
    "        txt_filename = os.path.splitext(filename)[0] + '.txt'\n",
    "        with open(os.path.join(output_dir, txt_filename), 'w') as f:\n",
    "            for ann in annotations:\n",
    "                f.write(ann + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting annotations for /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/bounding_boxes/train_labels.csv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 813/813 [00:00<00:00, 4154.74it/s]\n",
      "Converting annotations for /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/bounding_boxes/test_labels.csv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [00:00<00:00, 3523.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define class mapping\n",
    "class_mapping = {'Graffiti': 0}\n",
    "\n",
    "# Convert training annotations\n",
    "convert_annotations(TRAIN_CSV_FILE, TRAIN_IMAGES_DIR, TRAIN_LABELS_DIR, class_mapping)\n",
    "\n",
    "# Convert test annotations\n",
    "convert_annotations(TEST_CSV_FILE, TEST_IMAGES_DIR, TEST_LABELS_DIR, class_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_images(source_dir, dest_dir, num_images, used_images=set()):\n",
    "    images = [f for f in os.listdir(source_dir) if f.endswith('.jpg') and f not in used_images]\n",
    "    selected_images = random.sample(images, min(num_images, len(images)))\n",
    "    used_images.update(selected_images)\n",
    "\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "    for img in selected_images:\n",
    "        shutil.copy(os.path.join(source_dir, img), os.path.join(dest_dir, img))\n",
    "    return used_images\n",
    "\n",
    "def copy_annotation_files(image_dir, label_source_dir, label_dest_dir):\n",
    "    if not os.path.exists(label_dest_dir):\n",
    "        os.makedirs(label_dest_dir)\n",
    "    for img_file in os.listdir(image_dir):\n",
    "        if img_file.endswith('.jpg'):\n",
    "            base_name = os.path.splitext(img_file)[0]\n",
    "            label_file = base_name + '.txt'\n",
    "            src_label_path = os.path.join(label_source_dir, label_file)\n",
    "            dst_label_path = os.path.join(label_dest_dir, label_file)\n",
    "            if os.path.exists(src_label_path):\n",
    "                shutil.copy(src_label_path, dst_label_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize used images sets\n",
    "used_train_images = set()\n",
    "used_test_images = set()\n",
    "\n",
    "# Select 400 random training images\n",
    "used_train_images = select_random_images(TRAIN_IMAGES_DIR, SELECTED_TRAIN_IMAGES_DIR, 400, used_train_images)\n",
    "\n",
    "# Copy corresponding training annotation files\n",
    "copy_annotation_files(SELECTED_TRAIN_IMAGES_DIR, TRAIN_LABELS_DIR, SELECTED_TRAIN_LABELS_DIR)\n",
    "\n",
    "# Select 40 random test images\n",
    "used_test_images = select_random_images(TEST_IMAGES_DIR, SELECTED_TEST_IMAGES_DIR, 40, used_test_images)\n",
    "\n",
    "# Copy corresponding test annotation files\n",
    "copy_annotation_files(SELECTED_TEST_IMAGES_DIR, TEST_LABELS_DIR, SELECTED_TEST_LABELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yaml_file(file_path, train_images, val_images, nc, class_names):\n",
    "    data = {\n",
    "        'train': train_images,\n",
    "        'val': val_images,\n",
    "        'nc': nc,\n",
    "        'names': class_names\n",
    "    }\n",
    "    with open(file_path, 'w') as f:\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, list):\n",
    "                value_str = str(value).replace(\"'\", \"\")\n",
    "                f.write(f\"{key}: {value_str}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "train_images_path = os.path.abspath(SELECTED_TRAIN_IMAGES_DIR)\n",
    "val_images_path = os.path.abspath(SELECTED_TEST_IMAGES_DIR)  # Using test data as validation set\n",
    "\n",
    "nc = 1\n",
    "class_names = ['Graffiti']\n",
    "\n",
    "create_yaml_file(yaml_file_path, train_images_path, val_images_path, nc, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=week-06-portfolio/models/yolov5su.pt, data=/Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/graffiti.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=mps, workers=8, project=None, name=graffiti_detection7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/runs/detect/graffiti_detection7\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n",
      " 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
      " 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n",
      " 24        [17, 20, 23]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "YOLOv5s summary: 262 layers, 9,122,579 parameters, 9,122,563 gradients, 24.0 GFLOPs\n",
      "\n",
      "Transferred 421/427 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/runs/detect/graffiti_detection7', view at http://localhost:6006/\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/labels/train_selected... 400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:00<00:00, 2470.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/labels/train_selected.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/labels/test_selected... 40 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<00:00, 2944.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/labels/test_selected.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/runs/detect/graffiti_detection7/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/runs/detect/graffiti_detection7\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G       1.52      2.984      1.502         38        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:04<00:00,  2.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.473        0.5      0.388      0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.441      2.086      1.391         30        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:48<00:00,  1.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.258      0.275      0.153     0.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G       1.63      2.029       1.53         40        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:49<00:00,  1.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.129        0.3      0.118     0.0569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.623      1.955      1.545         58        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:28<00:00,  3.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.184      0.287       0.13     0.0469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.669      1.833      1.526         46        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:55<00:00,  2.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80        0.2        0.2      0.161     0.0915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.588      1.727      1.481         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:09<00:00,  2.80s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.237      0.212      0.121     0.0554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.568      1.734      1.509         48        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:09<00:00,  2.80s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.621       0.15      0.155     0.0726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.441      1.582      1.404         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:01<00:00,  2.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.736      0.209      0.241      0.141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.346      1.426      1.314         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:04<00:00,  2.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.479       0.15      0.143     0.0979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.286      1.368      1.265         32        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:06<00:00,  2.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.544        0.2      0.192      0.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.224 hours.\n",
      "Optimizer stripped from /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/runs/detect/graffiti_detection7/weights/last.pt, 18.5MB\n",
      "Optimizer stripped from /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/runs/detect/graffiti_detection7/weights/best.pt, 18.5MB\n",
      "\n",
      "Validating /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/runs/detect/graffiti_detection7/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.92 ðŸš€ Python-3.11.5 torch-2.4.0 MPS (Apple M1 Pro)\n",
      "YOLOv5s summary (fused): 193 layers, 9,111,923 parameters, 0 gradients, 23.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.408      0.212      0.168      0.104\n",
      "Speed: 3.7ms preprocess, 87.9ms inference, 0.0ms loss, 158.3ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/runs/detect/graffiti_detection7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLOv5s model\n",
    "model = YOLO('week-06-portfolio/models/yolov5su.pt')  # Ensure yolov5s.pt is in your current directory or provide the correct path\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=yaml_file_path, epochs=10, imgsz=640, batch=16, name='graffiti_detection', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(pred_boxes, true_boxes):\n",
    "    # Convert boxes to tensors\n",
    "    pred_boxes = torch.tensor(pred_boxes)\n",
    "    true_boxes = torch.tensor(true_boxes)\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = box_iou(pred_boxes, true_boxes)\n",
    "    return iou.diag().numpy()  # Get IoUs for matched boxes\n",
    "\n",
    "def evaluate_model(model, images_dir, labels_dir, output_images_dir=None):\n",
    "    results = []\n",
    "    images = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    if output_images_dir and not os.path.exists(output_images_dir):\n",
    "        os.makedirs(output_images_dir)\n",
    "\n",
    "    for img_name in tqdm(images, desc='Evaluating model'):\n",
    "        img_path = os.path.join(images_dir, img_name)\n",
    "        label_path = os.path.join(labels_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "\n",
    "        # Perform inference\n",
    "        preds = model.predict(img_path, conf=0.25)\n",
    "\n",
    "        # Load image for drawing\n",
    "        img = cv2.imread(img_path)\n",
    "        img_height, img_width = img.shape[:2]\n",
    "\n",
    "        # Get predicted boxes and confidence scores\n",
    "        pred_boxes = []\n",
    "        confidences = []\n",
    "        for pred in preds:\n",
    "            for box in pred.boxes:\n",
    "                x_min = box.xyxy[0][0].item()\n",
    "                y_min = box.xyxy[0][1].item()\n",
    "                x_max = box.xyxy[0][2].item()\n",
    "                y_max = box.xyxy[0][3].item()\n",
    "                conf = box.conf.item()\n",
    "                pred_boxes.append([x_min, y_min, x_max, y_max])\n",
    "                confidences.append(conf)\n",
    "\n",
    "                # Draw predicted bounding box\n",
    "                if output_images_dir:\n",
    "                    label = f\"{model.names[int(box.cls)]}: {conf:.2f}\"\n",
    "                    cv2.rectangle(img, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 2)\n",
    "                    cv2.putText(img, label, (int(x_min), int(y_min) - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Get true boxes\n",
    "        true_boxes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
    "                    # Convert back to absolute coordinates\n",
    "                    x_center *= img_width\n",
    "                    y_center *= img_height\n",
    "                    width *= img_width\n",
    "                    height *= img_height\n",
    "                    x_min = x_center - width / 2\n",
    "                    y_min = y_center - height / 2\n",
    "                    x_max = x_center + width / 2\n",
    "                    y_max = y_center + height / 2\n",
    "                    true_boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "                    # Draw true bounding box\n",
    "                    if output_images_dir:\n",
    "                        cv2.rectangle(img, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 0, 255), 2)\n",
    "\n",
    "        # Save image with drawn bounding boxes\n",
    "        if output_images_dir:\n",
    "            output_image_path = os.path.join(output_images_dir, img_name)\n",
    "            cv2.imwrite(output_image_path, img)\n",
    "\n",
    "        # Compute IoU\n",
    "        if pred_boxes and true_boxes:\n",
    "            ious = compute_iou(pred_boxes, true_boxes)\n",
    "            max_iou = max(ious)\n",
    "            max_conf = confidences[ious.argmax()]\n",
    "        else:\n",
    "            max_iou = 0.0\n",
    "            max_conf = 0.0 if not confidences else max(confidences)\n",
    "\n",
    "        results.append({\n",
    "            'image_name': img_name,\n",
    "            'confidence_value': max_conf,\n",
    "            'IoU_value': max_iou\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180713_105001.jpg: 640x480 13 Graffitis, 4247.7ms\n",
      "Speed: 141.5ms preprocess, 4247.7ms inference, 2023.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:   2%|â–Ž         | 1/40 [00:07<04:48,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20100101_032102.jpg: 640x480 14 Graffitis, 68.9ms\n",
      "Speed: 4.8ms preprocess, 68.9ms inference, 1592.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:   5%|â–Œ         | 2/40 [00:09<02:37,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20100101_022648.jpg: 640x480 6 Graffitis, 47.6ms\n",
      "Speed: 2.9ms preprocess, 47.6ms inference, 326.6ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:   8%|â–Š         | 3/40 [00:09<01:31,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180811_083858.jpg: 640x480 6 Graffitis, 26.6ms\n",
      "Speed: 2.8ms preprocess, 26.6ms inference, 396.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  10%|â–ˆ         | 4/40 [00:10<00:59,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20100101_031805.jpg: 640x480 1 Graffiti, 23.2ms\n",
      "Speed: 2.0ms preprocess, 23.2ms inference, 379.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  12%|â–ˆâ–Ž        | 5/40 [00:10<00:44,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180724_150758.jpg: 640x480 5 Graffitis, 26.9ms\n",
      "Speed: 2.3ms preprocess, 26.9ms inference, 347.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  15%|â–ˆâ–Œ        | 6/40 [00:11<00:34,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180723_145326.jpg: 640x480 6 Graffitis, 52.8ms\n",
      "Speed: 2.3ms preprocess, 52.8ms inference, 327.4ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  18%|â–ˆâ–Š        | 7/40 [00:11<00:26,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180714_103808.jpg: 640x480 1 Graffiti, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 422.9ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  20%|â–ˆâ–ˆ        | 8/40 [00:12<00:22,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180721_090151.jpg: 640x480 4 Graffitis, 25.5ms\n",
      "Speed: 1.9ms preprocess, 25.5ms inference, 355.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  22%|â–ˆâ–ˆâ–Ž       | 9/40 [00:12<00:20,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_1251.jpg: 640x480 3 Graffitis, 28.3ms\n",
      "Speed: 3.1ms preprocess, 28.3ms inference, 42.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [00:12<00:15,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180724_121832.jpg: 640x480 8 Graffitis, 27.8ms\n",
      "Speed: 2.0ms preprocess, 27.8ms inference, 461.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  28%|â–ˆâ–ˆâ–Š       | 11/40 [00:13<00:16,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180723_145121.jpg: 640x480 21 Graffitis, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 715.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [00:14<00:18,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180719_143627.jpg: 640x480 16 Graffitis, 17.6ms\n",
      "Speed: 2.1ms preprocess, 17.6ms inference, 465.7ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/40 [00:15<00:17,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180724_150507.jpg: 640x480 1 Graffiti, 17.7ms\n",
      "Speed: 2.1ms preprocess, 17.7ms inference, 7.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180720_142751.jpg: 640x480 3 Graffitis, 24.4ms\n",
      "Speed: 3.3ms preprocess, 24.4ms inference, 316.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [00:15<00:11,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180724_120949.jpg: 640x480 9 Graffitis, 26.6ms\n",
      "Speed: 1.9ms preprocess, 26.6ms inference, 7.7ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [00:15<00:08,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180811_083958.jpg: 640x480 7 Graffitis, 25.8ms\n",
      "Speed: 2.1ms preprocess, 25.8ms inference, 524.9ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/40 [00:16<00:10,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180724_123854.jpg: 640x480 5 Graffitis, 26.4ms\n",
      "Speed: 1.9ms preprocess, 26.4ms inference, 357.1ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [00:16<00:09,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180725_114304.jpg: 640x480 7 Graffitis, 26.8ms\n",
      "Speed: 1.8ms preprocess, 26.8ms inference, 6.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180720_141301.jpg: 640x480 6 Graffitis, 18.2ms\n",
      "Speed: 2.1ms preprocess, 18.2ms inference, 421.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [00:17<00:07,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180717_095144.jpg: 640x480 3 Graffitis, 26.1ms\n",
      "Speed: 2.0ms preprocess, 26.1ms inference, 7.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180720_123553.jpg: 640x480 8 Graffitis, 21.3ms\n",
      "Speed: 2.1ms preprocess, 21.3ms inference, 379.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [00:17<00:05,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20100101_023139.jpg: 640x480 3 Graffitis, 24.4ms\n",
      "Speed: 2.2ms preprocess, 24.4ms inference, 12.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180717_122331.jpg: 640x480 16 Graffitis, 17.8ms\n",
      "Speed: 1.9ms preprocess, 17.8ms inference, 361.7ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [00:18<00:04,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20100101_033117.jpg: 640x480 4 Graffitis, 20.1ms\n",
      "Speed: 2.0ms preprocess, 20.1ms inference, 332.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 25/40 [00:18<00:04,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180724_150928.jpg: 640x480 4 Graffitis, 23.5ms\n",
      "Speed: 2.1ms preprocess, 23.5ms inference, 7.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180721_090506.jpg: 640x480 4 Graffitis, 20.0ms\n",
      "Speed: 2.1ms preprocess, 20.0ms inference, 467.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 27/40 [00:19<00:03,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180723_124006.jpg: 640x480 4 Graffitis, 26.8ms\n",
      "Speed: 2.1ms preprocess, 26.8ms inference, 8.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180725_121940.jpg: 640x480 8 Graffitis, 18.5ms\n",
      "Speed: 2.1ms preprocess, 18.5ms inference, 432.2ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 29/40 [00:19<00:03,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180719_142631.jpg: 640x480 4 Graffitis, 32.5ms\n",
      "Speed: 1.9ms preprocess, 32.5ms inference, 480.9ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 30/40 [00:20<00:03,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180721_090501.jpg: 640x480 13 Graffitis, 26.7ms\n",
      "Speed: 1.9ms preprocess, 26.7ms inference, 601.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 31/40 [00:20<00:03,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_1302.jpg: 640x480 8 Graffitis, 29.8ms\n",
      "Speed: 1.8ms preprocess, 29.8ms inference, 393.3ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 32/40 [00:21<00:03,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180723_123915.jpg: 640x480 17 Graffitis, 29.7ms\n",
      "Speed: 1.8ms preprocess, 29.7ms inference, 664.1ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 33/40 [00:22<00:03,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_1261.jpg: 640x480 7 Graffitis, 27.1ms\n",
      "Speed: 2.1ms preprocess, 27.1ms inference, 14.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180725_120143.jpg: 640x480 5 Graffitis, 22.8ms\n",
      "Speed: 2.5ms preprocess, 22.8ms inference, 8.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 35/40 [00:22<00:01,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20100101_031404.jpg: 640x480 7 Graffitis, 26.9ms\n",
      "Speed: 2.1ms preprocess, 26.9ms inference, 392.6ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 36/40 [00:22<00:01,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180725_122231.jpg: 640x480 8 Graffitis, 28.1ms\n",
      "Speed: 2.6ms preprocess, 28.1ms inference, 12.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180720_142111.jpg: 640x480 17 Graffitis, 21.2ms\n",
      "Speed: 2.6ms preprocess, 21.2ms inference, 399.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/40 [00:23<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180724_121144.jpg: 640x480 7 Graffitis, 27.3ms\n",
      "Speed: 1.9ms preprocess, 27.3ms inference, 8.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "image 1/1 /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/images/test_selected/IMG_20180714_102857.jpg: 640x480 5 Graffitis, 27.6ms\n",
      "Speed: 1.7ms preprocess, 27.6ms inference, 34.8ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:23<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "df_results = evaluate_model(model, SELECTED_TEST_IMAGES_DIR, SELECTED_TEST_LABELS_DIR, OUTPUT_IMAGES_DIR)\n",
    "df_results.to_csv('week-06-portfolio/evaluation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting iteration 1\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=week-06-portfolio/models/yolov5su.pt, data=week-06-portfolio/yaml/graffiti_iter_1.yaml, epochs=5, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=mps, workers=8, project=week-06-portfolio/runs/train, name=graffiti_detection_iter_1, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=week-06-portfolio/runs/train/graffiti_detection_iter_1\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n",
      " 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
      " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
      " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
      " 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n",
      " 24        [17, 20, 23]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "YOLOv5s summary: 262 layers, 9,122,579 parameters, 9,122,563 gradients, 24.0 GFLOPs\n",
      "\n",
      "Transferred 421/427 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir week-06-portfolio/runs/train/graffiti_detection_iter_1', view at http://localhost:6006/\n",
      "Freezing layer 'model.24.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/week-06-portfolio/labels/train_selected_iter_1... 400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:00<00:00, 3062.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/week-06-portfolio/labels/train_selected_iter_1.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/anhdang/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/datasets/11/labels/test_selected.cache... 40 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to week-06-portfolio/runs/train/graffiti_detection_iter_1/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mweek-06-portfolio/runs/train/graffiti_detection_iter_1\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.518      2.467      1.449         85        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [03:18<00:00,  7.94s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 3.600s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 2.400s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         40         80      0.439      0.113     0.0955     0.0653\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.468      1.898      1.338         96        640:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [02:19<01:49,  9.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     model \u001b[38;5;241m=\u001b[39m YOLO(previous_model_path)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Train the model with 5 epochs\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myaml_file_path_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Lowered epochs to 5 as per your earlier request\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweek-06-portfolio/runs/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgraffiti_detection_iter_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43miteration\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     56\u001b[0m output_images_dir_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweek-06-portfolio/evaluation_images_iter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/ultralytics/engine/model.py:815\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/ultralytics/engine/trainer.py:208\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/ultralytics/engine/trainer.py:386\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    385\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m world_size\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py:105\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/ultralytics/nn/tasks.py:287\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m    286\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/ultralytics/utils/loss.py:232\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Pboxes\u001b[39;00m\n\u001b[1;32m    230\u001b[0m pred_bboxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_decode(anchor_points, pred_distri)  \u001b[38;5;66;03m# xyxy, (b, h*w, 4)\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m _, target_bboxes, target_scores, fg_mask, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massigner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m target_scores_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(target_scores\u001b[38;5;241m.\u001b[39msum(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Cls loss\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/ultralytics/utils/tal.py:72\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.forward\u001b[0;34m(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt)\u001b[0m\n\u001b[1;32m     63\u001b[0m     device \u001b[38;5;241m=\u001b[39m gt_bboxes\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     65\u001b[0m         torch\u001b[38;5;241m.\u001b[39mfull_like(pd_scores[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbg_idx)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     66\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(pd_bboxes)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(pd_scores[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m mask_pos, align_metric, overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pos_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manc_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m target_gt_idx, fg_mask, mask_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_highest_overlaps(mask_pos, overlaps, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_max_boxes)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Assigned target\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/ultralytics/utils/tal.py:94\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.get_pos_mask\u001b[0;34m(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt)\u001b[0m\n\u001b[1;32m     92\u001b[0m mask_in_gts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_candidates_in_gts(anc_points, gt_bboxes)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Get anchor_align metric, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m align_metric, overlaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_box_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_in_gts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Get topk_metric mask, (b, max_num_obj, h*w)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m mask_topk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_topk_candidates(align_metric, topk_mask\u001b[38;5;241m=\u001b[39mmask_gt\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopk)\u001b[38;5;241m.\u001b[39mbool())\n",
      "File \u001b[0;32m~/Documents/Github/COS40007-Artificial-Intelligence-for-Engineering/venv/lib/python3.11/site-packages/ultralytics/utils/tal.py:116\u001b[0m, in \u001b[0;36mTaskAlignedAssigner.get_box_metrics\u001b[0;34m(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, mask_gt)\u001b[0m\n\u001b[1;32m    113\u001b[0m bbox_scores[mask_gt] \u001b[38;5;241m=\u001b[39m pd_scores[ind[\u001b[38;5;241m0\u001b[39m], :, ind[\u001b[38;5;241m1\u001b[39m]][mask_gt]  \u001b[38;5;66;03m# b, max_num_obj, h*w\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# (b, max_num_obj, 1, 4), (b, 1, h*w, 4)\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m pd_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mpd_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_max_boxes, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[mask_gt]\n\u001b[1;32m    117\u001b[0m gt_boxes \u001b[38;5;241m=\u001b[39m gt_bboxes\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, na, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[mask_gt]\n\u001b[1;32m    118\u001b[0m overlaps[mask_gt] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miou_calculation(gt_boxes, pd_boxes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize variables for iterative training\n",
    "iou_threshold = 0.9\n",
    "satisfied = False\n",
    "iteration = 1\n",
    "\n",
    "while not satisfied:\n",
    "    print(f\"\\nStarting iteration {iteration}\")\n",
    "\n",
    "    # Select new training images\n",
    "    training_images = random.sample(os.listdir(TRAIN_IMAGES_DIR), 400)\n",
    "\n",
    "    # Update training data directories\n",
    "    selected_train_images_dir = f'week-06-portfolio/images/train_selected_iter_{iteration}'\n",
    "    selected_train_labels_dir = f'week-06-portfolio/labels/train_selected_iter_{iteration}'\n",
    "    if not os.path.exists(selected_train_images_dir):\n",
    "        os.makedirs(selected_train_images_dir)\n",
    "    if not os.path.exists(selected_train_labels_dir):\n",
    "        os.makedirs(selected_train_labels_dir)\n",
    "\n",
    "    # Copy selected images and annotations\n",
    "    for img in training_images:\n",
    "        shutil.copy(os.path.join(TRAIN_IMAGES_DIR, img), os.path.join(selected_train_images_dir, img))\n",
    "        label_file = os.path.splitext(img)[0] + '.txt'\n",
    "        src_label_path = os.path.join(TRAIN_LABELS_DIR, label_file)\n",
    "        dst_label_path = os.path.join(selected_train_labels_dir, label_file)\n",
    "        if os.path.exists(src_label_path):\n",
    "            shutil.copy(src_label_path, dst_label_path)\n",
    "\n",
    "    # Update YAML file\n",
    "    train_images_path = os.path.abspath(selected_train_images_dir)\n",
    "    val_images_path = os.path.abspath(SELECTED_TEST_IMAGES_DIR)\n",
    "\n",
    "    # Update the YAML file path for this iteration\n",
    "    yaml_file_path_iter = f'week-06-portfolio/yaml/graffiti_iter_{iteration}.yaml'\n",
    "    create_yaml_file(yaml_file_path_iter, train_images_path, val_images_path, nc, class_names)\n",
    "\n",
    "    # Load the model from previous iteration\n",
    "    if iteration == 1:\n",
    "        model = YOLO('week-06-portfolio/models/yolov5su.pt')  # Start with pre-trained YOLOv5su model\n",
    "    else:\n",
    "        previous_model_path = f'week-06-portfolio/runs/train/graffiti_detection_iter_{iteration - 1}/weights/best.pt'\n",
    "        model = YOLO(previous_model_path)\n",
    "\n",
    "    # Train the model with 5 epochs\n",
    "    model.train(\n",
    "        data=yaml_file_path_iter,\n",
    "        epochs=5,  # Lowered epochs to 5 as per your earlier request\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        project='week-06-portfolio/runs/train',\n",
    "        name=f'graffiti_detection_iter_{iteration}',\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    output_images_dir_iter = f'week-06-portfolio/evaluation_images_iter_{iteration}'\n",
    "    if not os.path.exists(output_images_dir_iter):\n",
    "        os.makedirs(output_images_dir_iter)\n",
    "\n",
    "    df_results = evaluate_model(model, SELECTED_TEST_IMAGES_DIR, SELECTED_TEST_LABELS_DIR, output_images_dir_iter)\n",
    "    df_results.to_csv(f'week-06-portfolio/evaluation_results_iter_{iteration}.csv', index=False)\n",
    "\n",
    "    # Check IoU threshold\n",
    "    over_threshold = df_results[df_results['IoU_value'] > iou_threshold]\n",
    "    if len(over_threshold) / len(df_results) >= 0.8:\n",
    "        print(f\"IoU threshold met in iteration {iteration}\")\n",
    "        satisfied = True\n",
    "    else:\n",
    "        print(f\"IoU threshold not met in iteration {iteration}\")\n",
    "\n",
    "    # Prepare for next iteration\n",
    "    iteration += 1\n",
    "\n",
    "\n",
    "if not satisfied:\n",
    "    print(\"IoU threshold not met after all iterations.\")\n",
    "else:\n",
    "    print(\"Training completed successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
